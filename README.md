Task 1: Big Data Analysis using PySpark

Project Overview:

This project is part of the CodTech Data Analysis Internship task set. The goal of Task 1 was to perform scalable big data analysis using PySpark on a large dataset, demonstrating efficient data processing and deriving actionable insights.

Dataset Used:

Dataset: NYC Taxi Trip Data
Format: CSV
Size: 500,000+ rows (large enough for scalable processing)
Key Features:
VendorID, pickup_datetime, dropoff_datetime, passenger_count, trip_distance, fare_amount, RatecodeID, and more.

Tools & Libraries Used:

PySpark (for big data processing)
Pandas (for result conversion)
Matplotlib & Seaborn (for data visualization)
Jupyter Notebook (Anaconda)

Analysis Performed:

Total trip count calculation
Average trip distance measurement
Vendor-based trip distribution analysis
Hourly trip distribution (peak hours identification)
Hourly average fare analysis
Ratecode usage frequency analysis

Key Insights:

The dataset contained approximately X trips (replace with actual count).
Vendor 1 had the highest number of trips, indicating market preference.
The busiest hours for taxi trips were between X and Y hours.
Average fares were higher around hour Z, suggesting premium demand times.
RatecodeID X was the most frequently used fare type.

Learnings:

Hands-on experience with scalable data processing using PySpark.
Applying group aggregations, transformations, and datetime operations on large datasets.
Visualizing large data results to extract patterns and trends.
